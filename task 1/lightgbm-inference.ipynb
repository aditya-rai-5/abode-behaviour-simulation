{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n# CELL 1 (NEW): SETUP FOR STAGE 2\n# ==============================================================================\nprint(\"Installing LightGBM for our Stage 2 Regressor...\")\n!pip install lightgbm scikit-learn pandas vaderSentiment --quiet\n\n# We need all the libraries from both models\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom google.colab import drive\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom datasets import Dataset\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# --- Mount Drive and Load the FULL 300k Excel file ---\nprint(\"\\nMounting Google Drive...\")\ndrive.mount('/content/drive', force_remount=True)\n\ndef load_data_from_drive(filename=\"Copy of behaviour_simulation_train.xlsx\"):\n    file_path = f'/content/drive/MyDrive/{filename}'\n    try:\n        print(f\"\\nAttempting to load full training data from: {file_path}\")\n        df = pd.read_excel(file_path)\n        print(f\"Data loaded successfully! Shape: {df.shape}\")\n        df.rename(columns={'dates': 'date', 'inferred company': 'company'}, inplace=True)\n        df['date'] = pd.to_datetime(df['date'])\n        return df\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n# This is our full 300,000 row dataset\nfull_train_df = load_data_from_drive()","metadata":{"id":"D8RXaBjv6i7j","colab":{"base_uri":"https://localhost:8080/"},"outputId":"efd6a6d1-c1ac-499d-e9a7-f9a8e72e2265"},"outputs":[{"output_type":"stream","name":"stdout","text":["Installing LightGBM for our Stage 2 Regressor...\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\n","Mounting Google Drive...\n","Mounted at /content/drive\n","\n","Attempting to load full training data from: /content/drive/MyDrive/Copy of behaviour_simulation_train.xlsx\n","Data loaded successfully! Shape: (300000, 7)\n"]}],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 2 (NEW): LOAD YOUR STAGE 1 CLASSIFIER\n# ==============================================================================\n# Define the base model name (must match what you trained)\nBASE_MODEL_NAME = \"distilbert-base-uncased\"\n# This is the path to YOUR saved model weights in Google Drive\nSAVED_MODEL_PATH = \"/content/drive/MyDrive/my_best_tweet_model\"\n\nprint(f\"Loading tokenizer '{BASE_MODEL_NAME}' from Hugging Face Hub...\")\ntry:\n    # --- LOAD TOKENIZER FROM WEB ---\n    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n    print(\"Tokenizer loaded successfully from web.\")\n\n    # --- LOAD FINE-TUNED MODEL FROM DRIVE ---\n    print(f\"Loading fine-tuned Stage 1 Classifier from: {SAVED_MODEL_PATH}...\")\n    classifier_model = AutoModelForSequenceClassification.from_pretrained(SAVED_MODEL_PATH).to(device)\n    print(\"Stage 1 Classifier loaded successfully from Drive.\")\n\nexcept Exception as e:\n     print(f\"\\n*** AN UNEXPECTED ERROR OCCURRED: {e} ***\")","metadata":{"id":"12qCHUiu6mbS","colab":{"base_uri":"https://localhost:8080/","height":214,"referenced_widgets":["b14595197ed84c4cab56743b9c7d118e","97ee09e8fe6a40b885ae5c08ff04ac6e","25e875a13aa54938a031dc2e9f567fc0","5ef4debe79b64cc6848e638e7c5a7249","e127480a8a1b4645a566aebe6677c51c","7624a8b77796400e86a2c5e540eef50f","b7f5f9ad0e944f3f8056b579224d893c","88f2456844df49b9b244ed2962b3f324","166ad483dfbc4a43ab459925e8c59637","c2b92b3be3614d348f453dda2ebaa23f","b81e8447ecff4a849c88b6d81887fff3","127806c4d87b4c3fb6667830f1c20e7c","fc7132a0a71342cb8a4892ee106ff868","06233b3bf7504de1934adb3d30fc2ea1","c1f30c3f84d5415e8df559062493ce39","620070fed138432abce3d2f700d88cb0","661af883f4664afdb598fb6d55fc0bef","f7aec60c3c964106b6925df812756486","5c77275249014be284cb2081f8720738","7808883bf047457c9d72f99c4a18b680","4557c29828704949b3c6ab13075a6b43","04a40790ce544b56abdc32cb404f7ab1","b6bf5b3efac548f695175faa7e4a686d","d739be3a7634475cb3276a7423745492","0e5bb6ab1df34a059775c568a20296c1","3336cdc9e680465aa5bc4f55c371b37c","9178cd64a6df4b2dacb3ecfa369aab6c","d7acbda5a60e4e97b2b56864bfd57548","3d4adf3461964a63975dc5b5e28c28ea","8de4bb8c2a6c4729b15aeb571a41c464","bfcc902646574b64a3bfb64a311bd4fa","904b54e053d84b85a5d888c7e531dd90","6bca4dee37734e3b84fe267826e3f6e8","fadba5e4cca84c709cbfc4752eb1e9eb","3d85e04297584e0293196cd5c40ca97b","1514f4f9ab834681b2ad57a6533a57a7","fcf8a9c9b8b74f748c4e2903327e505a","fe8f018340124b5f8cfcc94c44574977","72aee46ce72e41b19706741c87149175","6ddd7ad7a2a1499182c49e243296b2b9","4e9d17864338472cae334d20c831bfa7","811fe65448ee432898447aad6df0b178","326110fd062b44bc8659f201b5dddc94","302cc6f1d9f549a29c98189b23257b81"]},"outputId":"e875d089-a11d-4280-d6b4-a7de61a2a9d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading tokenizer 'distilbert-base-uncased' from Hugging Face Hub...\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b14595197ed84c4cab56743b9c7d118e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"127806c4d87b4c3fb6667830f1c20e7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6bf5b3efac548f695175faa7e4a686d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fadba5e4cca84c709cbfc4752eb1e9eb"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Tokenizer loaded successfully from web.\n","Loading fine-tuned Stage 1 Classifier from: /content/drive/MyDrive/my_best_tweet_model...\n","Stage 1 Classifier loaded successfully from Drive.\n"]}],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 3 (NEW, CORRECTED): GENERATE ALL FEATURES FOR STAGE 2\n# ==============================================================================\n\n# --- 1. Define our helper functions ---\ndef format_input_text(row):\n    tweet_text = str(row['content']).strip()\n    company = str(row['company']).strip()\n    hour = row['date'].hour\n    day = row['date'].day_name()\n    has_media = \"yes\" if pd.notna(row['media']) else \"no\"\n    return f\"Brand: {company} | Day: {day} | Hour: {hour} | Media: {has_media} | Tweet: {tweet_text}\"\n\ndef create_manual_features(df):\n    print(\"Generating manual features (text length, time, sentiment)...\")\n    temp_df = df.copy()\n    analyzer = SentimentIntensityAnalyzer()\n\n    # Simple text features\n    temp_df['content'] = temp_df['content'].fillna('').astype(str)\n    temp_df['text_len'] = temp_df['content'].apply(len)\n    temp_df['word_count'] = temp_df['content'].apply(lambda x: len(x.split()))\n\n    # --- START OF FIX ---\n    # We convert categorical columns to the special 'category' dtype\n    # This ensures all splits (train/val) will know about all possible categories.\n    temp_df['hour'] = temp_df['date'].dt.hour.astype('category')\n    temp_df['dayofweek'] = temp_df['date'].dt.dayofweek.astype('category')\n    temp_df['has_media'] = temp_df['media'].notna().astype('category')\n\n    # For high-cardinality features like company/username, 'category' is the\n    # correct dtype for LightGBM to handle them efficiently.\n    temp_df['company_cat'] = temp_df['company'].astype('category')\n    temp_df['username_cat'] = temp_df['username'].astype('category')\n    # --- END OF FIX ---\n\n    # Sentiment\n    temp_df['sentiment'] = temp_df['content'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n\n    print(\"Manual features complete.\")\n    return temp_df\n\n# --- 2. Generate Transformer Probabilities (This is the slow part) ---\nprint(\"Generating formatted text for Transformer...\")\nfull_train_df['text'] = full_train_df.apply(format_input_text, axis=1)\n\n# Convert to a 'Dataset' to use the fast .map() function\nhf_dataset = Dataset.from_pandas(full_train_df[['text']])\n\ndef predict_probabilities(batch):\n    # Tokenize the batch of text\n    inputs = tokenizer(\n        batch['text'],\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=256\n    ).to(device)\n\n    # Get model outputs (logits)\n    with torch.no_grad():\n        logits = classifier_model(**inputs).logits\n\n    # Convert logits to probabilities using softmax\n    probabilities = torch.softmax(logits, dim=1).cpu().numpy()\n\n    # Return as a dictionary of new columns\n    return {\n        'prob_low': probabilities[:, 0],\n        'prob_medium': probabilities[:, 1],\n        'prob_high': probabilities[:, 2],\n        'prob_viral': probabilities[:, 3],\n    }\n\nprint(\"Running Stage 1 Classifier to generate probabilities for 300k tweets...\")\nprint(\"(This will take a significant amount of time, maybe 20-30 minutes)\")\nprob_dataset = hf_dataset.map(predict_probabilities, batched=True, batch_size=64)\nprint(\"Probability generation complete.\")\n\n# --- 3. Combine All Features ---\nprint(\"Combining all features...\")\n# Convert probability dataset back to pandas\nprob_df = prob_dataset.to_pandas()\n\n# Create manual features\nmanual_features_df = create_manual_features(full_train_df)\n\n# Define our feature names\nmanual_feature_cols = ['text_len', 'word_count', 'hour', 'dayofweek', 'sentiment', 'has_media', 'company_cat', 'username_cat']\nprob_feature_cols = ['prob_low', 'prob_medium', 'prob_high', 'prob_viral']\n\n# This is our final, feature-rich dataset for Stage 2\nX = pd.concat([\n    manual_features_df[manual_feature_cols],\n    prob_df[prob_feature_cols]\n], axis=1)\n\n# This is our target variable: log of likes\ny = np.log1p(full_train_df['likes'])\n\nprint(f\"Final training dataset 'X' created with shape: {X.shape}\")","metadata":{"id":"uy03TWtO6oML","colab":{"base_uri":"https://localhost:8080/","height":188,"referenced_widgets":["a7e48f98db4745d39e6089e5bc5f4746","755d67efdd6e47c3931d3a35a8236553","4f00c12a5a9e45afb506aabda1b4e6df","e796f6b20e164b5f81485d8cd3f0734d","4c16732fe78344b6a4b4433b35b822d2","0def6120e8ba4c7d8d3c2b8962ab271a","68f1562959074ebfbcba4be132b59677","4b376a4481eb4ee9ad4e61a33d760836","75c882fc2d0843c8bac2ca397c35f44d","add0a4c124334deaa48126bb8432c1e1","6f59f4fc974c46858f0612e6b235a407"]},"outputId":"7558466b-a202-4536-83a4-4ba39df5ef0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Generating formatted text for Transformer...\n","Running Stage 1 Classifier to generate probabilities for 300k tweets...\n","(This will take a significant amount of time, maybe 20-30 minutes)\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/300000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7e48f98db4745d39e6089e5bc5f4746"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Probability generation complete.\n","Combining all features...\n","Generating manual features (text length, time, sentiment)...\n","Manual features complete.\n","Final training dataset 'X' created with shape: (300000, 12)\n"]}],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 4 (NEW): TRAIN STAGE 2 REGRESSOR (LightGBM)\n# ==============================================================================\n\nprint(\"Training Stage 2 LightGBM Regressor...\")\n\n# We need to tell LightGBM which features are categorical\ncategorical_features = ['has_media', 'company_cat', 'username_cat', 'dayofweek', 'hour']\n\n# Split our new dataset for training and validation\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create the LightGBM model\nlgbm_regressor = lgb.LGBMRegressor(\n    objective='regression_l1', # L1 (MAE) is robust to outliers\n    metric='rmse',\n    n_estimators=1000,\n    learning_rate=0.05,\n    num_leaves=31,\n    random_state=42,\n    n_jobs=-1\n)\n\n# Train the model\nlgbm_regressor.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    eval_metric='rmse',\n    callbacks=[lgb.early_stopping(100)],\n    categorical_feature=categorical_features\n)\n\n# --- Evaluate our new model ---\nprint(\"\\n--- Stage 2 Model Evaluation ---\")\nval_preds_log = lgbm_regressor.predict(X_val)\n\n# Convert log predictions back to the real number of likes\nval_preds_real = np.expm1(val_preds_log)\ny_val_real = np.expm1(y_val)\n\n# Ensure no negative predictions\nval_preds_real[val_preds_real < 0] = 0\n\n# Calculate the final RMSE! This is your Task 1 score.\nfinal_rmse = np.sqrt(mean_squared_error(y_val_real, val_preds_real))\nprint(f\"Final Model RMSE (on real 'likes' scale): {final_rmse:.4f}\")","metadata":{"id":"xzOUWv5B6p3N","colab":{"base_uri":"https://localhost:8080/"},"outputId":"35f0bc68-fa30-48a8-972c-5a14e405360a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Stage 2 LightGBM Regressor...\n","[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n","[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022166 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 3252\n","[LightGBM] [Info] Number of data points in the train set: 240000, number of used features: 11\n","[LightGBM] [Info] Start training from score 4.343805\n","Training until validation scores don't improve for 100 rounds\n","Early stopping, best iteration is:\n","[450]\tvalid_0's rmse: 0.73472\n","\n","--- Stage 2 Model Evaluation ---\n","Final Model RMSE (on real 'likes' scale): 3931.6662\n"]}],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 5 (NEW): SAVE THE STAGE 2 REGRESSOR\n# ==============================================================================\nimport joblib\n\nREGRESSOR_PATH = \"/content/drive/MyDrive/my_lgbm_regressor.txt\"\nprint(f\"Saving Stage 2 Regressor to: {REGRESSOR_PATH}\")\n\n# Save the trained model\njoblib.dump(lgbm_regressor, REGRESSOR_PATH)\n\nprint(\"Stage 2 Regressor saved successfully.\")","metadata":{"id":"sAnXn-zY62MG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c77f2daf-2380-497c-fd05-b12707a9bdc8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saving Stage 2 Regressor to: /content/drive/MyDrive/my_lgbm_regressor.txt\n","Stage 2 Regressor saved successfully.\n"]}],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 5b: SAVE THE MISSING SCHEMA FILE\n# (Run this in your original TRAINING notebook)\n# ==============================================================================\nimport pandas as pd\n\ntry:\n    # 'X' is the \"master spreadsheet\" (features) you created in Cell 3\n    # We are just grabbing its \"blueprint\" (the columns and data types)\n    schema_to_save = X.dtypes\n\n    # Define the save path (must match the loading path)\n    SCHEMA_PATH = \"/content/drive/MyDrive/my_data_schema.pkl\"\n\n    # Save the schema blueprint to your Drive\n    schema_to_save.to_pickle(SCHEMA_PATH)\n\n    print(f\"\\n--- SUCCESS! ---\")\n    print(f\"File '{SCHEMA_PATH}' has been created.\")\n    print(\"You can now go back to your INFERENCE notebook and re-run the loading cell.\")\n\nexcept NameError:\n    print(f\"\\n*** ERROR: The 'X' dataframe is no longer in memory. ***\")\n    print(\"Please go back and re-run Cell 3 (the long feature generation cell) to recreate 'X'.\")\n    print(\"After that, run this cell (Cell 5b) again.\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f3QmptlNvmpS","outputId":"000767d2-3375-428d-d6b7-af8aa900678f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- SUCCESS! ---\n","File '/content/drive/MyDrive/my_data_schema.pkl' has been created.\n","You can now go back to your INFERENCE notebook and re-run the loading cell.\n"]}],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 6 (NEW): FINAL 2-STAGE INFERENCE\n# ==============================================================================\nimport joblib\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n# --- Load BOTH models ---\nprint(\"Loading Stage 1 (Classifier) and Stage 2 (Regressor)...\")\n\n# --- Model 1: Classifier (Transformer) ---\nSTAGE_1_PATH = \"/content/drive/MyDrive/my_best_tweet_model\"\nBASE_MODEL_NAME = \"distilbert-base-uncased\"\n# Ensure tokenizer is loaded from the hub consistently if needed, or from local if saved\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n    print(\"Tokenizer loaded successfully.\")\nexcept Exception as e:\n    print(f\"Error loading tokenizer: {e}\")\n    # Fallback or error handling\n\n# Ensure model is on the correct device\ntry:\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    classifier_model = AutoModelForSequenceClassification.from_pretrained(STAGE_1_PATH).to(device)\n    print(f\"Stage 1 Classifier loaded successfully on device: {device}\")\nexcept Exception as e:\n    print(f\"Error loading classifier model: {e}\")\n    # Fallback or error handling\n\n\n# --- Model 2: Regressor (LightGBM) ---\nSTAGE_2_PATH = \"/content/drive/MyDrive/my_lgbm_regressor.txt\"\ntry:\n    regressor_model = joblib.load(STAGE_2_PATH)\n    print(\"Stage 2 Regressor loaded successfully.\")\nexcept Exception as e:\n    print(f\"Error loading regressor model: {e}\")\n    # Fallback or error handling\n\n\nprint(\"All models loaded successfully.\")\n\n# --- Regenerate Factorizers and Mappings for Categorical Features ---\n# This is necessary to ensure consistency with training data encoding\nprint(\"Regenerating factorizers and mappings for categorical features...\")\n# Use the full training dataframe to get all possible categories\ncompany_factorizer_train = pd.factorize(full_train_df['company'])\nusername_factorizer_train = pd.factorize(full_train_df['username'])\ndayofweek_train_levels = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nhour_train_levels = list(range(24))\nhas_media_train_levels = [0, 1] # 0 for no, 1 for yes\n\n# Create mappings from original value to factorized ID\ncompany_to_id = {val: id for id, val in enumerate(company_factorizer_train[1])}\nusername_to_id = {val: id for id, val in enumerate(username_factorizer_train[1])}\ndayofweek_to_id = {day: i for i, day in enumerate(dayofweek_train_levels)}\nhour_to_id = {i: i for i in hour_train_levels}\n\n\n# Define categorical dtypes using the levels from the training data\n# These dtypes are useful for creating the DataFrame with correct categories\ncompany_dtype = pd.CategoricalDtype(categories=company_factorizer_train[1])\nusername_dtype = pd.CategoricalDtype(categories=username_factorizer_train[1])\ndayofweek_dtype = pd.CategoricalDtype(categories=dayofweek_train_levels)\nhour_dtype = pd.CategoricalDtype(categories=hour_train_levels)\nhas_media_dtype = pd.CategoricalDtype(categories=has_media_train_levels)\n\n\nprint(\"Factorizers and categorical dtypes regenerated.\")\n\n# --- Define categorical feature names for LightGBM prediction ---\ncategorical_feature_names_for_predict = ['has_media', 'company_cat', 'username_cat', 'dayofweek', 'hour']\n\n\n# --- Define the full prediction pipeline ---\ndef predict_likes_twostage(tweet_text, company, username, day, hour, has_media):\n\n    # --- 1. Create Stage 1 Input (Transformer) ---\n    formatted_text = f\"Brand: {company} | Day: {day} | Hour: {hour} | Media: {has_media} | Tweet: {tweet_text}\"\n    inputs = tokenizer(formatted_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(device)\n\n    # --- 2. Get Stage 1 Probabilities ---\n    with torch.no_grad():\n        logits = classifier_model(**inputs).logits\n    probabilities = torch.softmax(logits, dim=1).cpu().numpy()[0] # Get the 1D array of 4 probs\n\n    # --- 3. Create Stage 2 Input (LGBM) ---\n    # We need to create the *exact same* manual features as our training set\n    manual_features = {}\n    manual_features['text_len'] = len(tweet_text)\n    manual_features['word_count'] = len(tweet_text.split())\n\n    # Map input values to factorized IDs using the training mappings\n    manual_features['hour'] = hour_to_id.get(hour, -1) # Use .get with a default for safety\n    manual_features['dayofweek'] = dayofweek_to_id.get(day, -1) # Use .get with a default for safety\n    manual_features['sentiment'] = SentimentIntensityAnalyzer().polarity_scores(tweet_text)['compound']\n    manual_features['has_media'] = 1 if has_media == \"yes\" else 0 # Map 'yes'/'no' to 1/0\n\n    # Use the regenerated factorizers for company and username\n    manual_features['company_cat'] = company_to_id.get(company, -1) # Use .get with a default for safety\n    manual_features['username_cat'] = username_to_id.get(username, -1) # Use .get with a default for safety\n\n    # Create the feature DataFrame\n    # Column order must be EXACTLY as it was in training (Cell 3)\n    feature_names = ['text_len', 'word_count', 'hour', 'dayofweek', 'sentiment',\n                     'has_media', 'company_cat', 'username_cat',\n                     'prob_low', 'prob_medium', 'prob_high', 'prob_viral']\n\n    # Combine manual features and probability features\n    all_features = [\n        manual_features['text_len'], manual_features['word_count'], manual_features['hour'],\n        manual_features['dayofweek'], manual_features['sentiment'], manual_features['has_media'],\n        manual_features['company_cat'], manual_features['username_cat'],\n        probabilities[0], probabilities[1], probabilities[2], probabilities[3]\n    ]\n\n    # Create a 2D array for the single prediction\n    X_pred = pd.DataFrame([all_features], columns=feature_names)\n\n    # Explicitly set categorical feature types using the training levels\n    # This helps pandas align categories but might not be enough for LGBM predict directly\n    X_pred['has_media'] = X_pred['has_media'].astype(has_media_dtype)\n    X_pred['company_cat'] = X_pred['company_cat'].astype(company_dtype)\n    X_pred['username_cat'] = X_pred['username_cat'].astype(username_dtype)\n    X_pred['dayofweek'] = X_pred['dayofweek'].astype(dayofweek_dtype)\n    X_pred['hour'] = X_pred['hour'].astype(hour_dtype)\n\n\n    # --- 4. Get Stage 2 Prediction (Log scale) ---\n    # Pass categorical_feature names explicitly to the predict method\n    pred_log = regressor_model.predict(X_pred, categorical_feature=categorical_feature_names_for_predict)[0]\n\n    # --- 5. Convert to Real Likes ---\n    pred_real = np.expm1(pred_log)\n    pred_real = max(0, pred_real) # Ensure no negative likes\n\n    return pred_real\n\n# --- Run a Final Test ---\nprint(\"\\n--- Running a test 2-Stage prediction ---\")\ntest_tweet = \"SAY HIS NAME!! #CannonHinnant Some low life ghetto Thug Murdered a shot a 5 year old boy in the head!It took the mainstream media a week to talk about it! SO SAD!THIS IS WHY I SAY ALL LIVES MATTERCannon Hinnant life mattered too! Say his name <hyperlink>\"\ntest_company = \"williams\" # Use the actual company name\ntest_day = \"Friday\"\ntest_hour = 23\ntest_media = \"yes\"\ntest_username = \"w_terrence\" # Use the actual username\n\n# Check if the test company and username exist in the training data's factorizers\nif test_company not in company_to_id:\n    print(f\"Warning: Test company '{test_company}' not found in training data. Using placeholder ID (-1).\")\nif test_username not in username_to_id:\n     print(f\"Warning: Test username '{test_username}' not found in training data. Using placeholder ID (-1).\")\nif test_day not in dayofweek_to_id:\n     print(f\"Warning: Test day '{test_day}' not found in dayofweek mapping. Using placeholder ID (-1).\")\nif test_hour not in hour_to_id:\n     print(f\"Warning: Test hour '{test_hour}' not found in hour mapping. Using placeholder ID (-1).\")\n\n\nfinal_likes_prediction = predict_likes_twostage(\n    test_tweet,\n    test_company,\n    test_username,\n    test_day,\n    test_hour,\n    test_media\n)\n\nprint(f\"\\n---> Final Predicted Likes: {final_likes_prediction:.0f}\")","metadata":{"id":"7ePz53i369ht","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4d4f857e-7fc8-4570-b4e5-03145707903d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading Stage 1 (Classifier) and Stage 2 (Regressor)...\n","Tokenizer loaded successfully.\n","Stage 1 Classifier loaded successfully on device: cuda\n","Stage 2 Regressor loaded successfully.\n","All models loaded successfully.\n","Regenerating factorizers and mappings for categorical features...\n","Factorizers and categorical dtypes regenerated.\n","\n","--- Running a test 2-Stage prediction ---\n","\n","---> Final Predicted Likes: 17663\n"]}],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 6 (NEW, CORRECTED): FINAL 2-STAGE INFERENCE (Fixed fillna)\n# ==============================================================================\nimport joblib\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n# --- Load BOTH models AND the schema ---\nprint(\"Loading Stage 1 (Classifier), Stage 2 (Regressor), and Schema...\")\n\n# --- Model 1: Classifier (Transformer) ---\nSTAGE_1_PATH = \"/content/drive/MyDrive/my_best_tweet_model\"\nBASE_MODEL_NAME = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\nclassifier_model = AutoModelForSequenceClassification.from_pretrained(STAGE_1_PATH).to(device)\n\n# --- Model 2: Regressor (LightGBM) ---\nSTAGE_2_PATH = \"/content/drive/MyDrive/my_lgbm_regressor.txt\"\nregressor_model = joblib.load(STAGE_2_PATH)\n\n# --- Schema (The Dtypes) ---\nSCHEMA_PATH = \"/content/drive/MyDrive/my_data_schema.pkl\"\ntrain_schema = pd.read_pickle(SCHEMA_PATH)\n\nprint(\"All models and schema loaded successfully.\")\n\n# --- Define the full prediction pipeline ---\ndef predict_likes_twostage_hybrid(tweet_text, company, username, day, hour, has_media,\n                                    classifier_threshold=0.90, safe_low_prediction=50.0):\n\n    # --- 1. Create Stage 1 Input (Transformer) ---\n    formatted_text = f\"Brand: {company} | Day: {day} | Hour: {hour} | Media: {has_media} | Tweet: {tweet_text}\"\n    inputs = tokenizer(formatted_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(device)\n\n    # --- 2. Get Stage 1 Probabilities ---\n    with torch.no_grad():\n        logits = classifier_model(**inputs).logits\n    probabilities = torch.softmax(logits, dim=1).cpu().numpy()[0] # Get the 1D array of 4 probs\n\n    prob_low = probabilities[0]\n\n    # --- 3. *** THE HYBRID LOGIC *** ---\n    if prob_low > classifier_threshold:\n        return safe_low_prediction # Return a safe, low number\n\n    # --- 4. ELSE, use the \"Genius\" Regressor for Medium/High/Viral tweets ---\n\n    # Create Stage 2 Input (LGBM)\n    data = {\n        'text_len': len(tweet_text),\n        'word_count': len(tweet_text.split()),\n        'hour': hour,\n        'dayofweek': pd.to_datetime(f\"2023-01-02 {hour}:00:00\").dayofweek,\n        'sentiment': SentimentIntensityAnalyzer().polarity_scores(tweet_text)['compound'],\n        'has_media': 1 if has_media == \"yes\" else 0,\n        'company_cat': company,\n        'username_cat': username,\n        'prob_low': probabilities[0],\n        'prob_medium': probabilities[1],\n        'prob_high': probabilities[2],\n        'prob_viral': probabilities[3]\n    }\n\n    X_pred = pd.DataFrame(data, index=[0], columns=train_schema.index)\n\n    # Apply the saved dtypes (with all known categories)\n    for col, dtype in train_schema.items():\n        if str(dtype) == 'category':\n            X_pred[col] = pd.Categorical(X_pred[col], categories=dtype.categories)\n        else:\n            X_pred[col] = X_pred[col].astype(dtype)\n\n    # --- THIS IS THE FIX ---\n    # We cannot fill 'NaN' with 0 for all columns.\n    # 'company_cat' and 'username_cat' are categorical and will crash\n    # if we try to fill their NaN (unknown company) with the integer 0.\n    # LightGBM is smart and can handle NaNs in categorical features perfectly.\n\n    # So, we only fillna(0) for the *non-categorical* columns.\n    numerical_cols = train_schema[train_schema != 'category'].index\n\n    # This fills NaNs with 0 only for columns like text_len, sentiment, probs, etc.\n    # It leaves the NaNs in company_cat and username_cat, which is what LGBM wants.\n    X_pred[numerical_cols] = X_pred[numerical_cols].fillna(0)\n    # --- END OF FIX ---\n\n    # --- 5. Get Stage 2 Prediction (Log scale) ---\n    pred_log = regressor_model.predict(X_pred)[0]\n\n    # --- 6. Convert to Real Likes ---\n    pred_real = np.expm1(pred_log)\n    pred_real = max(0, pred_real) # Ensure no negative likes\n\n    return pred_real\n\n# --- Run a Final Test ---\nprint(\"\\n--- Running a test with the FINAL HYBRID prediction logic ---\")\n\ntest_tweet = \"SAY HIS NAME!! #CannonHinnant Some low life ghetto Thug Murdered a shot a 5 year old boy in the head!It took the mainstream media a week to talk about it! SO SAD!THIS IS WHY I SAY ALL LIVES MATTERCannon Hinnant life mattered too! Say his name <hyperlink>\"\ntest_company = \"williams\" # Use the actual company name\ntest_day = \"Friday\"\ntest_hour = 23\ntest_media = \"yes\"\ntest_username = \"w_terrence\" # Use the actual username\n\nfinal_likes_prediction = predict_likes_twostage_hybrid(\n    test_tweet,\n    test_company,\n    test_username,\n    test_day,\n    test_hour,\n    test_media\n)\n\nprint(f\"\\n---> Final Predicted Likes: {final_likes_prediction:.0f}\")\n\n# --- Test a \"boring\" tweet ---\nprint(\"\\n--- Running a test on a 'boring' tweet ---\")\nboring_tweet = \"Our Q3 earnings are in line with expectations. See the full report here: [link]\"\nboring_company = \"Microsoft\"\nboring_day = \"Tuesday\"\nboring_hour = 8\nboring_media = \"no\"\nboring_username = \"Microsoft\"\n\nboring_likes_prediction = predict_likes_twostage_hybrid(\n    boring_tweet,\n    boring_company,\n    boring_username,\n    boring_day,\n    boring_hour,\n    boring_media\n)\nprint(f\"\\n---> Final Predicted Likes: {boring_likes_prediction:.0f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pz1JlRqvu93H","outputId":"97afc35c-ff3a-40bc-b864-2bb2851a8185"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading Stage 1 (Classifier), Stage 2 (Regressor), and Schema...\n","All models and schema loaded successfully.\n","\n","--- Running a test with the FINAL HYBRID prediction logic ---\n","\n","---> Final Predicted Likes: 16673\n","\n","--- Running a test on a 'boring' tweet ---\n","\n","---> Final Predicted Likes: 192\n"]}],"execution_count":null}]}