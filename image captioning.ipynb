{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13546454,"sourceType":"datasetVersion","datasetId":8603200},{"sourceId":13566287,"sourceType":"datasetVersion","datasetId":8617647},{"sourceId":13566454,"sourceType":"datasetVersion","datasetId":8617779}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:34:35.430426Z","iopub.execute_input":"2025-10-31T10:34:35.430655Z","iopub.status.idle":"2025-10-31T10:34:35.776934Z","shell.execute_reply.started":"2025-10-31T10:34:35.430629Z","shell.execute_reply":"2025-10-31T10:34:35.776185Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install -q transformers accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:34:35.778359Z","iopub.execute_input":"2025-10-31T10:34:35.778654Z","iopub.status.idle":"2025-10-31T10:34:39.189004Z","shell.execute_reply.started":"2025-10-31T10:34:35.778636Z","shell.execute_reply":"2025-10-31T10:34:39.188180Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:34:39.190063Z","iopub.execute_input":"2025-10-31T10:34:39.190378Z","iopub.status.idle":"2025-10-31T10:34:42.566503Z","shell.execute_reply.started":"2025-10-31T10:34:39.190329Z","shell.execute_reply":"2025-10-31T10:34:42.565574Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.48.2)\nRequirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.19.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.9.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nfrom transformers import AutoProcessor, Qwen2VLForConditionalGeneration, BitsAndBytesConfig\n\nmodel_path = \"Qwen/Qwen2-VL-7B-Instruct\"\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type='nf4'\n)\n\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    quantization_config=quantization_config\n)\n\nprocessor = AutoProcessor.from_pretrained(model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:34:42.567605Z","iopub.execute_input":"2025-10-31T10:34:42.567859Z","iopub.status.idle":"2025-10-31T10:36:05.088480Z","shell.execute_reply.started":"2025-10-31T10:34:42.567826Z","shell.execute_reply":"2025-10-31T10:36:05.087639Z"}},"outputs":[{"name":"stderr","text":"2025-10-31 10:34:47.212328: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761906887.235201     418 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761906887.242146     418 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddca6d7ab10f4d7ebef203eb470b8480"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nYou have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# text_prompt=\"\"\"\n# You are an image analysis system. Describe the image with maximum objective detail. Report only what is visible. Do not guess hidden context.\n\n# Required fields:\n# 1. Objects:\n#    - List all visible objects.\n#    - Note size, shape, color, position, and count.\n\n# 2. Environment:\n#    - Indoor or outdoor.\n#    - Background elements.\n#    - Surfaces, textures, lighting, shadows.\n\n# 3. People (if present):\n#    - Count.\n#    - Visible clothing details.\n#    - Actions or poses.\n#    - Visible facial expressions strictly from appearance.\n\n# 4. Vehicle details (if present):\n#    - Make, model badges, color.\n#    - Body style.\n#    - Wheel design, lights, grill pattern, mirrors.\n\n# 5. Text/Logos:\n#    - Extract any visible text or branding exactly as seen.\n\n# 6. Scene description:\n#    - Orientation, camera angle, perspective.\n#    - Depth cues, reflections, highlights.\n\n# 7. Visual mood indicators:\n#    - Lighting type (harsh, soft, directional).\n#    - Color temperature (warm, cool).\n#    - Only from observable visual cues.\n\n# 8. Spatial relationships:\n#    - Foreground, midground, background positions.\n#    - Relative sizes.\n\n# 9. Materials:\n#    - Surface finish (matte, glossy, metallic).\n#    - Texture patterns.\n\n# Constraints:\n# - Do not infer intent, backstory, or emotions beyond visible facial geometry.\n# - No opinions or assumptions.\n# - Use technical and concise language.\n# - Output should be long and highly detailed.\n# - Use complete sentences in a single structured paragraph.\n# \"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:36:05.089523Z","iopub.execute_input":"2025-10-31T10:36:05.090310Z","iopub.status.idle":"2025-10-31T10:36:05.094767Z","shell.execute_reply.started":"2025-10-31T10:36:05.090277Z","shell.execute_reply":"2025-10-31T10:36:05.093907Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# def caption_image_qwen(image_file, prompt):\n#     \"\"\"\n#     Generates a caption for a given image file (URL or local path)\n#     using the loaded Qwen-VL model and a text prompt.\n#     \"\"\"\n#     # Load the image\n#     if image_file.startswith('http') or image_file.startswith('https'):\n#         response = requests.get(image_file)\n#         image = Image.open(BytesIO(response.content)).convert('RGB')\n#     else:\n#         image = Image.open(image_file).convert('RGB')\n        \n#     # 1. Create the conversation prompt\n#     # Qwen-VL uses a list of dictionaries for the conversation.\n#     conversation = [\n#         {\n#             \"role\": \"user\",\n#             \"content\": [\n#                 {\"type\": \"image\"},\n#                 {\"type\": \"text\", \"text\": prompt}\n#             ]\n#         }\n#     ]\n    \n#     # 2. Apply the chat template and process the inputs\n#     # The processor prepares both the text and the image.\n#     text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n    \n#     # Process the text and image together\n#     inputs = processor(\n#         text=[text_prompt],  # Note: text is passed as a list\n#         images=[image],    # Note: images are passed as a list\n#         return_tensors=\"pt\"\n#     ).to(model.device)\n\n#     # 3. Generate the caption\n#     with torch.inference_mode():\n#         output_ids = model.generate(**inputs, max_new_tokens=1024)\n\n#     outputs = processor.batch_decode(output_ids, skip_special_tokens=True)\n    \n#     # Clean the output\n#     cleaned_output = outputs[0].split(\"\\n\")[-1].strip()\n#     return cleaned_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:36:05.095599Z","iopub.execute_input":"2025-10-31T10:36:05.096137Z","iopub.status.idle":"2025-10-31T10:36:05.109473Z","shell.execute_reply.started":"2025-10-31T10:36:05.096118Z","shell.execute_reply":"2025-10-31T10:36:05.108732Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# image_url = \"https://pbs.twimg.com/media/Eo8N3JLVoAAlDJT?format=jpg&name=small\"\n\n# caption = caption_image_qwen(image_url, text_prompt)\n\n# print(\"--- Generated Caption ---\")\n# print(caption)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:36:05.110214Z","iopub.execute_input":"2025-10-31T10:36:05.110496Z","iopub.status.idle":"2025-10-31T10:36:05.122893Z","shell.execute_reply.started":"2025-10-31T10:36:05.110473Z","shell.execute_reply":"2025-10-31T10:36:05.122234Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def thumb_extract(url):\n  \"\"\"\n  Extracts the actual image URL from the media string.\n  e.g., \"Photo(thumbnailUrl='http://...')\" -> \"http://...\"\n  \"\"\"\n  try:\n    return str(url).split(\"'\")[1]\n  except Exception:\n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:36:05.124687Z","iopub.execute_input":"2025-10-31T10:36:05.125198Z","iopub.status.idle":"2025-10-31T10:36:05.132296Z","shell.execute_reply.started":"2025-10-31T10:36:05.125180Z","shell.execute_reply":"2025-10-31T10:36:05.131608Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def caption_images_batch(image_files_list, prompt):\n    \"\"\"\n    Generates captions for a BATCH of image files (URLs or local paths)\n    using the loaded Qwen-VL model and a single shared prompt.\n    \"\"\"\n    images = []\n    max_size = (512, 512) \n    \n    for image_file in image_files_list:\n        try:\n            if image_file.startswith('http') or image_file.startswith('https'):\n                response = requests.get(image_file, timeout=10) \n                current_image = Image.open(BytesIO(response.content)).convert('RGB')\n            else:\n                current_image = Image.open(image_file).convert('RGB')\n            \n            # --- WHAT I'VE ADDED ---\n            # Resize the image in-place while maintaining aspect ratio\n            # This makes processing much faster and uses less memory\n            current_image.thumbnail(max_size, Image.Resampling.LANCZOS)\n            images.append(current_image)\n            # --- END OF ADDITION ---\n            \n        except Exception as e:\n            print(f\"Error loading image {image_file}: {e}\")\n            images.append(None) \n\n    # --- The rest of the function is the same ---\n    \n    valid_images = [img for img in images if img is not None]\n    \n    if not valid_images:\n        return [\"Error: No valid images in batch\"] * len(image_files_list)\n\n    conversations = []\n    for _ in valid_images:\n        conversations.append([\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\"},\n                    {\"type\": \"text\", \"text\": prompt}\n                ]\n            }\n        ])\n\n    text_prompts = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)\n    \n    inputs = processor(\n        text=text_prompts,\n        images=valid_images,\n        return_tensors=\"pt\",\n        padding=True \n    ).to(model.device)\n\n    with torch.inference_mode():\n        output_ids = model.generate(\n            **inputs, \n            max_new_tokens=512,\n            do_sample=False\n        )\n\n    input_token_len = inputs['input_ids'].shape[1]\n    generated_ids = output_ids[:, input_token_len:]\n    responses = processor.batch_decode(generated_ids, skip_special_tokens=True)\n    \n    cleaned_responses = [resp.strip() for resp in responses]\n    \n    final_output = []\n    resp_index = 0\n    for img in images:\n        if img is None:\n            final_output.append(\"Error: Image load failed\")\n        else:\n            final_output.append(cleaned_responses[resp_index])\n            resp_index += 1\n            \n    return final_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:36:05.132964Z","iopub.execute_input":"2025-10-31T10:36:05.133135Z","iopub.status.idle":"2025-10-31T10:36:05.142950Z","shell.execute_reply.started":"2025-10-31T10:36:05.133122Z","shell.execute_reply":"2025-10-31T10:36:05.142395Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nimport time\nimport math\n\nBATCH_SIZE = 32\nYOUR_CSV_FILE = '/kaggle/input/behaviour-simulation/content_simulation_train.csv'\nOUTPUT_CSV_FILE = '/kaggle/working/generated_captions.csv'\nMEDIA_COLUMN_NAME = 'media'\nNO_MEDIA_STRING = \"no media found\"\n\ntext_prompt=\"\"\"\nDescribe everything visible in the image with maximum objective detail, including all objects, environment, people, text, vehicle features, materials, lighting, spatial layout, textures, and reflections without guessing unseen context or intent\"\"\"\n\nprint(\"Loading dataset from {YOUR_CSV_FILE}...\")\ntry:\n    df = pd.read_csv(YOUR_CSV_FILE)\nexcept FileNotFoundError:\n    print(f\"Error: File not found at {YOUR_CSV_FILE}\")\n\ndf['generated_caption'] = None\nprint(f\"Dataset loaded. Total rows: {len(df)}\")\n\ntotal_batches = math.ceil(len(df) / BATCH_SIZE)\ntotal_start_time = time.time()\n\nprint(f\"Starting processing in batches of {BATCH_SIZE}...\")\n\nfor i in range(0, len(df), BATCH_SIZE):\n    batch_start_time = time.time()\n    \n    batch_df = df.iloc[i : i + BATCH_SIZE]\n    \n    urls_to_process = []\n    valid_indices = []\n\n    for index, row in batch_df.iterrows():\n        media_data = str(row[\"media\"])\n        \n        if NO_MEDIA_STRING not in media_data.lower() and pd.notna(media_data):\n            extracted_url = thumb_extract(media_data)\n            if extracted_url:\n                urls_to_process.append(extracted_url)\n                valid_indices.append(index) # यह इंडेक्स (e.g., 20, 21, 23, 27...)\n            else:\n                df.at[index, 'generated_caption'] = \"Error: URL extraction failed\"\n        else:\n            df.at[index, 'generated_caption'] = \"\"\n\n    if urls_to_process:\n        print(f\"Processing Batch {i//BATCH_SIZE + 1}/{total_batches} (Found {len(urls_to_process)} valid images)...\")\n        \n        batch_captions = caption_images_batch(urls_to_process, text_prompt)\n        \n        for j, caption in enumerate(batch_captions):\n            original_index = valid_indices[j]\n            df.at[original_index, 'generated_caption'] = caption\n    else:\n        print(f\"Skipping Batch {i//BATCH_SIZE + 1}/{total_batches} (No valid images found)\")\n\n    batch_end_time = time.time()\n    print(f\"Batch {i//BATCH_SIZE + 1} finished in {batch_end_time - batch_start_time:.2f} seconds.\")\n\n    if (i // BATCH_SIZE) % 5 == 0:\n        print(f\"Saving intermediate results to {OUTPUT_CSV_FILE}...\")\n        df.to_csv(OUTPUT_CSV_FILE, index=False)\n\n\ntotal_end_time = time.time()\nprint(\"\\n--- Processing Complete ---\")\nprint(f\"Total time taken: {(total_end_time - total_start_time) / 60:.2f} minutes.\")\nprint(f\"Saving final results to {OUTPUT_CSV_FILE}...\")\ndf.to_csv(OUTPUT_CSV_FILE, index=False)\n\nprint(\"Done.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T10:36:05.143613Z","iopub.execute_input":"2025-10-31T10:36:05.143855Z"}},"outputs":[{"name":"stdout","text":"Loading dataset from {YOUR_CSV_FILE}...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}